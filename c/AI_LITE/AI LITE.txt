AI_LITE

The general focus of AI_LITE is to create a light weight language model without prior detailed knowledge of how language models (LM) work. This project has been a joy to create since the beginning and I have learned a lot of how to improve artificial intelligence.

THE BEGINNING (TWITTERBOT)

Before I had the idea of creating a language model, when I was about 20 years old working at Walmart, I pondered about how AI works. Since OpenAI's GPT-3 model hit the center stage of the world, I could only wonder as to how it worked. I had only little knowledge of how neural networks worked. I heard that the GPT model utilized this which peaked my couriosity.
While working at Walmart, there were moments where I had some down time between customers in the electronic section. Between these moments, I would walk back and forth through out the enitre section, thinking of how such a thing would work. More than ever since I recently purchased my first laptop and "completed" my first CS class.
At the beginning of 2021, I began work after a few weeks of thought of how such a thing would work. Using java, I completed my first working model under an hour. Most of that time, if I recall correctly, was used towards collecting data off of the internet and I ended up scraping 'copy-pasta' off of Reddit.
My thought process working towards this moment was this: 
Neurons have connections to each other, but how?
How can I create these connections?
How can I create sentences with these connections?
I thought to answer the first question first and allow myself to answer the other questions. First, how can I create these neurons? I thought about how words can be used as neurons. At the time, I thought how primitive this was comparing my own ideas to what was being said about AI at the time.
This thought carried into the major question of how such data could be organized, which took most of my time thinking at work. How can I create these connections between words? Eventually I thought that I can itterate through a text file using Java's scanner and creating a txt file and store each word that came after it. If it existed already, it would open that .txt file and write the next word.
Again, the time it took to code this first attempt took less than an hour. However, it took weeks and months of planning the architecture of the program. This first version was called the "TwitterBot" as I thought about using it to create tweets on twitter.
"TwitterBot" uses a directory based system to create word data and a manifest txt file to store possible sentence starters. This, as you can imagine, lead to the creation of about ten thousand txt documents in one directory filled with txt documents. Since this is storage based, the learning process was a bit more managable since the data could be updated with more weights. Inside each of these txt documents was a list of possible next words, which in a sense act as links inside the directory database. 
The way that this version of AI_LITE, "TwitterBot" worked was that the user provides a word to the program and the AI sets out to create sentences from that word. This doesn't quite explain how simple this really was so let me provide an example of this process:
If I, the user, were to provide the word "The", the AI would look for that word in its database and begin creating a sentence from there. Simple, not very complex unfortunately. Each word would be selected randomly in the words next word list, leading to very unusual sentences that don't make much sense the majority of the time.
Since the issue of sentences not being of any quality most of the time, this lead into the next phase of development and the rise of "TwitterBot gen 2" where words can be clustered together to provide a much needed improvement to how the AI selects words. This version also incorporated the manifest to superficially generate random sentences without the need for user input.
These are the very beginnings of AI_LITE, and how it kinda works.


Next Generations

Taking things further, the "TwitterBot" evolved into "UselessTextGenerator" or "UTG" for short. This was mainly inspired by what data was scraped from the internet as most of what it was generating was heavily influenced from the chaotic nature of using copy pastas. A user interface was created to help sort out the happenings of what was going on. This was about the time when I learned Object Oriented Programming and applied this to the UTG. This greatly improved database compile time as all word objects were memory based and not directory based. 
Thus, the WordObject.java was born, holding all information about the word and its connections. Primitive at the time, this greatly improved the time it took to create sentences and allowed for further complex processes to happen since I wasn't bound to creating endless txt documents in a directory.
As life happened, I wasn't able to keep up the work for my own AI, but I didn't stop thinking about how to improve the inner workings of how it worked. I eventually learned more programming techniques that were applied over time but progress was slow for a long time.

The next true generation of this project wasn't labeled as a new thing, but rather an improved version of what was already there. I didn't know exactly what to call the new inner workings, but I setteled on naming them after certain fantasy things such as castle, legion, soldier, country, kingdom, etc. This list was expounded upon in later versions, specificaly in the C++ version and not the java version. With the newly created object class "SentenceData", I can store more data with WordObjects and have two waves of learning the data. This eventually fazed out the word clustering as it took way more memory than it needed to as the AI eventually was developed to look at a new list of sentences.
The first wave of learning remained largly the same with the exception of cutting out the clustering. Then the second wave would learn from each sentence and create SentenceData based on what it found. Since all associated word objects were created, there is no reason to be expecting unknown words and to search the database in memory for each word and add them to the SentenceData object. Then these would be added to a list for generation. The AI now is optimized and ready for much more accurate and meaningful sentence generation.
The new way that the AI will now generate sentences is that it will start with a randomly chosen word from the manifest and search the entire sentence database for any matches. It would then return a list of found sentences, which will be used as a pool of references for what words to be chosen next. This leads into another problem as sentence search would eventually return empty. Then we go back to the sentence list and itterate through each one to see if there is a small similarity, usualy about the four most resent words in the sentence, allowing it to continue. This also leads into another problem because this also will return an empty list, which we then would look into the last word in the sentence and grab a random next word to continue. This does eventually catch back into sentences that match the most recent four words. Again, another problem arises from this as the sentence may suddenly start talking about something else and become inconsistent. 
Everything talked about to this point is written in java. As you may know, Java isn't exactly the best language when it comes to memory management. Learning from about 3 MB of text data resulted in about 3 GB of ram usage which is very limiting and frustrating. Eventually I was able to port the code into C++ because of a performance and memory wall.

THE C++ VERSION AND FURTHER DEVELOPMENT

The process to port the java code to C++ was a journey in of itself. I learned much about how to manage memory in C++ and was able to get a successful working version that closely resembled a more sophisticated version of TwitterBot gen 2. Once I had everything working, including sentence data, I tested to see if there was much of a difference in how fast it generated sentences and how much memory it used. I was shocked by the results. About 1.5 MB of text data was used for the data. Here are the results:
Time to compile: C++ took about 40 MS while Java took 2.436 seconds.
Memory used using about 1.5 MB of data: C++ took about 31.4 MB while Java took about 2.6 GB.
It is plain to see that C++ is clearly better suited for this kind of project, allowing for better AI context and data structures. 
